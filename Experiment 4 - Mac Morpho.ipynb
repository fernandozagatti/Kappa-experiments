{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e7da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def agreement_calculation(df, name_columns):\n",
    "    results = []\n",
    "\n",
    "    for i in name_columns:\n",
    "        results.append(list(df[i]))\n",
    "    \n",
    "    unique = list(set([x for xs in results for x in xs]))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(unique)\n",
    "\n",
    "    if(len(results) <= 2):\n",
    "\n",
    "        encod_rater1 = le.transform(results[0])\n",
    "        encod_rater2 = le.transform(results[1])\n",
    "        kappa_cohen = cohen_kappa_score(encod_rater1, encod_rater2)\n",
    "\n",
    "        print(\"Kappa Cohen: \", kappa_cohen, \"\\n\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Kappa Cohen:  There are more than two annotators! In this case, use Kappa Fleiss or Krippendorff's Alpha.\\n\")\n",
    "\n",
    "    encod_rater = []\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        encod_rater.append(le.transform(results[i]))\n",
    "\n",
    "    giro = np.array(encod_rater).transpose()\n",
    "    kappa_fleiss = irr.fleiss_kappa(irr.aggregate_raters(giro)[0], method='fleiss')\n",
    "\n",
    "    print(\"Kappa Fleiss: \", kappa_fleiss, \"\\n\")\n",
    "    \n",
    "    # Krippendorff's Alpha commented due to hardware limitations\n",
    "\n",
    "    #alpha = krippendorff.alpha(reliability_data=results, level_of_measurement=\"nominal\")\n",
    "\n",
    "    #print(\"Krippendorff's alpha for nominal metric: \", alpha, \"\\n\")\n",
    "    \n",
    "    return kappa_cohen\n",
    "\n",
    "\n",
    "def extract_features(tagged_sentence, index):\n",
    "    token, tag = tagged_sentence[index]\n",
    "    prev_token = \"\"\n",
    "    if index > 0:\n",
    "        prev_token, prev_tag = tagged_sentence[index - 1]\n",
    "    is_number = False\n",
    "    try:\n",
    "        if float(token):\n",
    "            is_number = True\n",
    "    except:\n",
    "        pass\n",
    "    features_dict = {\"token\": token\n",
    "        , \"lower_cased_token\": token.lower()\n",
    "        , \"prev_token\": prev_token\n",
    "        , \"suffix1\": token[-1]\n",
    "        , \"suffix2\": token[-2:]\n",
    "        , \"suffix3\": token[-3:]\n",
    "        , \"is_capitalized\": token.upper() == token\n",
    "        , \"is_number\": is_number}\n",
    "    return features_dict\n",
    "\n",
    "\n",
    "def new_tuple_list(df_agreement):\n",
    "    sentence_temp = df_agreement['sentence'].tolist()\n",
    "    words_temp = df_agreement['words'].tolist()\n",
    "    target_temp = df_agreement['rater'].tolist()\n",
    "    tuple_list = []\n",
    "\n",
    "    for i in list(set(sentence_temp)):\n",
    "        tuple_list.append([])\n",
    "\n",
    "    for i in range(len(sentence_temp)):\n",
    "        tuple_list[sentence_temp[i]].append((words_temp[i], target_temp[i]))\n",
    "    \n",
    "    return tuple_list\n",
    "\n",
    "\n",
    "def trainer_fixed_pipeline(tuple_list, tuple_list_test):\n",
    "\n",
    "    X_features = []\n",
    "    for sentence in tuple_list:\n",
    "        for k in range(len(sentence)):\n",
    "            X_features.append(json.dumps(extract_features(sentence, k)))\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=10000)\n",
    "    X = vectorizer.fit_transform(X_features)\n",
    "    \n",
    "    Y = []\n",
    "    for sentence in tuple_list:\n",
    "        for k in range(len(sentence)):\n",
    "            Y.append(sentence[k][1])\n",
    "            \n",
    "    test_strings = []\n",
    "\n",
    "    for i in range(len(tuple_list_test)):\n",
    "        output_string = \" \".join(tuples[0] for tuples in tuple_list_test[i])\n",
    "        test_strings.append(output_string)\n",
    "    \n",
    "    ################################################\n",
    "    # RF\n",
    "    ################################################\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=10).fit(X, Y)\n",
    "    \n",
    "    y_pred_rf = []\n",
    "\n",
    "    for i in range(len(test_strings)):\n",
    "        tokens = test_strings[i].split()\n",
    "        null_tags = [(token, 'NULL') for token in tokens]\n",
    "        input = [json.dumps(extract_features(null_tags, i)) for i in range(len(null_tags))]\n",
    "        X_test = vectorizer.transform(input)\n",
    "        y_pred_rf.append(list(zip(tokens, rf.predict(X_test))))\n",
    "        \n",
    "    flat_predicted_tags = [tags for sentence in y_pred_rf for tags in sentence]\n",
    "    flat_true_tags = [tags for sentence in tuple_list_test for tags in sentence]\n",
    "\n",
    "    y_pred_rf = []\n",
    "    target = []\n",
    "    for i in range(len(flat_true_tags)):\n",
    "        y_pred_rf.append(flat_predicted_tags[i][1])\n",
    "        target.append(flat_true_tags[i][1])\n",
    "        \n",
    "    print(\"Classification report RF: \\n\", classification_report(target, y_pred_rf))\n",
    "    \n",
    "    ################################################\n",
    "    # NB\n",
    "    ################################################\n",
    "    \n",
    "    nb = MultinomialNB().fit(X, Y)\n",
    "    \n",
    "    y_pred_nb = []\n",
    "\n",
    "    for i in range(len(test_strings)):\n",
    "        tokens = test_strings[i].split()\n",
    "        null_tags = [(token, 'NULL') for token in tokens]\n",
    "        input = [json.dumps(extract_features(null_tags, i)) for i in range(len(null_tags))]\n",
    "        X_test = vectorizer.transform(input)\n",
    "        y_pred_nb.append(list(zip(tokens, nb.predict(X_test))))\n",
    "        \n",
    "    flat_predicted_tags = [tags for sentence in y_pred_nb for tags in sentence]\n",
    "    flat_true_tags = [tags for sentence in tuple_list_test for tags in sentence]\n",
    "\n",
    "    y_pred_nb = []\n",
    "    target = []\n",
    "    for i in range(len(flat_true_tags)):\n",
    "        y_pred_nb.append(flat_predicted_tags[i][1])\n",
    "        target.append(flat_true_tags[i][1])\n",
    "        \n",
    "    print(\"Classification report NB: \\n\", classification_report(target, y_pred_nb))\n",
    "    \n",
    "    ################################################\n",
    "    # LR\n",
    "    ################################################\n",
    "    \n",
    "    lr = LogisticRegression().fit(X, Y)\n",
    "    \n",
    "    y_pred_lr = []\n",
    "\n",
    "    for i in range(len(test_strings)):\n",
    "        tokens = test_strings[i].split()\n",
    "        null_tags = [(token, 'NULL') for token in tokens]\n",
    "        input = [json.dumps(extract_features(null_tags, i)) for i in range(len(null_tags))]\n",
    "        X_test = vectorizer.transform(input)\n",
    "        y_pred_lr.append(list(zip(tokens, lr.predict(X_test))))\n",
    "        \n",
    "    flat_predicted_tags = [tags for sentence in y_pred_lr for tags in sentence]\n",
    "    flat_true_tags = [tags for sentence in tuple_list_test for tags in sentence]\n",
    "\n",
    "    y_pred_lr = []\n",
    "    target = []\n",
    "    for i in range(len(flat_true_tags)):\n",
    "        y_pred_lr.append(flat_predicted_tags[i][1])\n",
    "        target.append(flat_true_tags[i][1])\n",
    "        \n",
    "    print(\"Classification report LR: \\n\", classification_report(target, y_pred_lr))\n",
    "    \n",
    "    return y_pred_rf, y_pred_nb, y_pred_lr, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecb9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('datasets/macmorpho/macmorpho-train.txt', 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "    \n",
    "name_columns = [\n",
    "    'target',\n",
    "    'rater'\n",
    "]\n",
    "\n",
    "# Split each string into a list of words\n",
    "words_list = [s.split() for s in data]\n",
    "\n",
    "# Split each word into a tuple of word and grammar type\n",
    "tuple_list = [[tuple(p.split('_')) for p in words] for words in words_list]\n",
    "\n",
    "class_list = ['ADJ', 'PREP+PROSUB', 'N', 'ART', 'PREP+ART', 'PREP', 'IN', 'PU', 'CUR', 'PROSUB', 'ADV',\n",
    "             'PREP+PROPESS', 'V', 'PDEN', 'NPROP', 'PROPESS', 'KC', 'ADV-KS', 'KS', 'NUM', 'PRO-KS',\n",
    "             'PREP+PROADJ', 'PCP', 'PREP+ADV', 'PROADJ', 'PREP+PRO-KS']\n",
    "\n",
    "# Load the test dataset\n",
    "with open('datasets/macmorpho/macmorpho-test.txt', 'r') as f:\n",
    "    test = f.read().splitlines()\n",
    "\n",
    "# Split each string into a list of words\n",
    "words_list_test = [s.split() for s in test]\n",
    "\n",
    "# Split each word into a tuple of word and grammar type\n",
    "tuple_list_test = [[tuple(p.split('_')) for p in words] for words in words_list_test]\n",
    "\n",
    "# For viewing at the end of all training\n",
    "agreements = ['< 0', 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "rf_f1 = []\n",
    "nb_f1 = []\n",
    "lr_f1 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf263f5",
   "metadata": {},
   "source": [
    "# Agreement 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515d081d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  1.0 \n",
      "\n",
      "Kappa Fleiss:  1.0 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.85      0.75      0.80      8554\n",
      "         ADV       0.81      0.80      0.80      5446\n",
      "      ADV-KS       0.78      0.56      0.65       230\n",
      "         ART       0.76      0.83      0.80     12580\n",
      "         CUR       0.89      0.84      0.86       296\n",
      "          IN       0.49      0.26      0.34        98\n",
      "          KC       0.81      0.55      0.65      4531\n",
      "          KS       0.85      0.65      0.74      2538\n",
      "           N       0.88      0.90      0.89     36542\n",
      "       NPROP       0.81      0.73      0.77     15936\n",
      "         NUM       0.83      0.86      0.84      2541\n",
      "         PCP       0.91      0.93      0.92      3640\n",
      "        PDEN       0.85      0.77      0.81      1092\n",
      "        PREP       0.92      0.93      0.93     16778\n",
      "    PREP+ADV       0.93      0.87      0.90        31\n",
      "    PREP+ART       0.94      0.99      0.96     10219\n",
      " PREP+PRO-KS       0.80      0.07      0.13        58\n",
      " PREP+PROADJ       0.95      0.96      0.95       309\n",
      "PREP+PROPESS       0.98      0.89      0.93       126\n",
      " PREP+PROSUB       0.75      0.40      0.52       156\n",
      "      PRO-KS       0.78      0.87      0.82      2195\n",
      "      PROADJ       0.86      0.91      0.88      3419\n",
      "     PROPESS       0.91      0.91      0.91      2876\n",
      "      PROSUB       0.81      0.41      0.54      1567\n",
      "          PU       0.90      0.99      0.95     26904\n",
      "           V       0.95      0.93      0.94     19711\n",
      "\n",
      "    accuracy                           0.88    178373\n",
      "   macro avg       0.85      0.75      0.78    178373\n",
      "weighted avg       0.88      0.88      0.87    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.74      0.70      8554\n",
      "         ADV       0.63      0.77      0.70      5446\n",
      "      ADV-KS       0.80      0.47      0.59       230\n",
      "         ART       0.87      0.56      0.68     12580\n",
      "         CUR       0.84      0.81      0.82       296\n",
      "          IN       0.50      0.02      0.04        98\n",
      "          KC       0.65      0.23      0.34      4531\n",
      "          KS       0.65      0.56      0.60      2538\n",
      "           N       0.82      0.76      0.79     36542\n",
      "       NPROP       0.77      0.62      0.69     15936\n",
      "         NUM       0.67      0.85      0.75      2541\n",
      "         PCP       0.73      0.85      0.79      3640\n",
      "        PDEN       0.54      0.80      0.64      1092\n",
      "        PREP       0.90      0.86      0.88     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.87      0.98      0.92     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.87      0.98      0.92       309\n",
      "PREP+PROPESS       1.00      0.71      0.83       126\n",
      " PREP+PROSUB       1.00      0.32      0.49       156\n",
      "      PRO-KS       0.66      0.90      0.76      2195\n",
      "      PROADJ       0.77      0.87      0.81      3419\n",
      "     PROPESS       0.79      0.92      0.85      2876\n",
      "      PROSUB       0.63      0.38      0.47      1567\n",
      "          PU       0.74      0.97      0.84     26904\n",
      "           V       0.86      0.87      0.87     19711\n",
      "\n",
      "    accuracy                           0.79    178373\n",
      "   macro avg       0.70      0.65      0.64    178373\n",
      "weighted avg       0.79      0.79      0.78    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.73      0.60      0.66      8554\n",
      "         ADV       0.70      0.75      0.72      5446\n",
      "      ADV-KS       0.46      0.66      0.54       230\n",
      "         ART       0.63      0.82      0.71     12580\n",
      "         CUR       0.87      0.81      0.84       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.69      0.27      0.39      4531\n",
      "          KS       0.82      0.38      0.52      2538\n",
      "           N       0.81      0.83      0.82     36542\n",
      "       NPROP       0.71      0.59      0.65     15936\n",
      "         NUM       0.79      0.75      0.77      2541\n",
      "         PCP       0.82      0.84      0.83      3640\n",
      "        PDEN       0.89      0.63      0.74      1092\n",
      "        PREP       0.89      0.91      0.90     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.91      0.99      0.95     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.96      0.83      0.89       309\n",
      "PREP+PROPESS       1.00      0.60      0.75       126\n",
      " PREP+PROSUB       1.00      0.28      0.43       156\n",
      "      PRO-KS       0.65      0.90      0.75      2195\n",
      "      PROADJ       0.83      0.83      0.83      3419\n",
      "     PROPESS       0.87      0.89      0.88      2876\n",
      "      PROSUB       0.78      0.38      0.51      1567\n",
      "          PU       0.89      0.98      0.93     26904\n",
      "           V       0.90      0.89      0.89     19711\n",
      "\n",
      "    accuracy                           0.81    178373\n",
      "   macro avg       0.72      0.63      0.65    178373\n",
      "weighted avg       0.81      0.81      0.81    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_100 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "if kappa_100 == 1:\n",
    "    y_pred_rf100, y_pred_nb100, y_pred_lr100, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf100, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb100, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr100, average='macro'))\n",
    "\n",
    "    del y_pred_rf100, y_pred_nb100, y_pred_lr100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4424073",
   "metadata": {},
   "source": [
    "# Agreement 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f5d08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.8867565652437752 \n",
      "\n",
      "Kappa Fleiss:  0.8867024929888037 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.85      0.72      0.78      8554\n",
      "         ADV       0.80      0.79      0.80      5446\n",
      "      ADV-KS       0.46      0.56      0.50       230\n",
      "         ART       0.70      0.83      0.76     12580\n",
      "         CUR       0.36      0.81      0.50       296\n",
      "          IN       0.06      0.20      0.10        98\n",
      "          KC       0.82      0.53      0.64      4531\n",
      "          KS       0.85      0.64      0.73      2538\n",
      "           N       0.88      0.87      0.88     36542\n",
      "       NPROP       0.80      0.71      0.75     15936\n",
      "         NUM       0.84      0.82      0.83      2541\n",
      "         PCP       0.91      0.90      0.90      3640\n",
      "        PDEN       0.63      0.77      0.69      1092\n",
      "        PREP       0.91      0.92      0.91     16778\n",
      "    PREP+ADV       0.18      0.87      0.30        31\n",
      "    PREP+ART       0.93      0.96      0.95     10219\n",
      " PREP+PRO-KS       0.08      0.05      0.06        58\n",
      " PREP+PROADJ       0.86      0.93      0.89       309\n",
      "PREP+PROPESS       0.68      0.88      0.77       126\n",
      " PREP+PROSUB       0.18      0.39      0.25       156\n",
      "      PRO-KS       0.75      0.86      0.80      2195\n",
      "      PROADJ       0.87      0.90      0.88      3419\n",
      "     PROPESS       0.77      0.91      0.84      2876\n",
      "      PROSUB       0.79      0.42      0.54      1567\n",
      "          PU       0.90      0.98      0.94     26904\n",
      "           V       0.95      0.92      0.93     19711\n",
      "\n",
      "    accuracy                           0.86    178373\n",
      "   macro avg       0.69      0.74      0.69    178373\n",
      "weighted avg       0.86      0.86      0.86    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.72      0.69      8554\n",
      "         ADV       0.63      0.78      0.70      5446\n",
      "      ADV-KS       0.35      0.52      0.42       230\n",
      "         ART       0.61      0.46      0.53     12580\n",
      "         CUR       0.38      0.83      0.52       296\n",
      "          IN       0.08      0.14      0.11        98\n",
      "          KC       0.64      0.22      0.33      4531\n",
      "          KS       0.68      0.61      0.64      2538\n",
      "           N       0.83      0.71      0.76     36542\n",
      "       NPROP       0.78      0.60      0.68     15936\n",
      "         NUM       0.67      0.82      0.74      2541\n",
      "         PCP       0.76      0.84      0.80      3640\n",
      "        PDEN       0.41      0.74      0.53      1092\n",
      "        PREP       0.85      0.86      0.85     16778\n",
      "    PREP+ADV       0.43      0.87      0.57        31\n",
      "    PREP+ART       0.87      0.99      0.92     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.84      0.97      0.90       309\n",
      "PREP+PROPESS       0.88      0.88      0.88       126\n",
      " PREP+PROSUB       0.16      0.33      0.21       156\n",
      "      PRO-KS       0.68      0.88      0.77      2195\n",
      "      PROADJ       0.76      0.89      0.82      3419\n",
      "     PROPESS       0.71      0.90      0.79      2876\n",
      "      PROSUB       0.65      0.36      0.47      1567\n",
      "          PU       0.74      0.96      0.84     26904\n",
      "           V       0.84      0.84      0.84     19711\n",
      "\n",
      "    accuracy                           0.76    178373\n",
      "   macro avg       0.61      0.68      0.63    178373\n",
      "weighted avg       0.77      0.76      0.75    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.73      0.56      0.64      8554\n",
      "         ADV       0.69      0.76      0.73      5446\n",
      "      ADV-KS       0.63      0.47      0.54       230\n",
      "         ART       0.58      0.83      0.68     12580\n",
      "         CUR       0.71      0.81      0.75       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.62      0.22      0.32      4531\n",
      "          KS       0.86      0.43      0.57      2538\n",
      "           N       0.79      0.83      0.81     36542\n",
      "       NPROP       0.76      0.56      0.64     15936\n",
      "         NUM       0.81      0.65      0.72      2541\n",
      "         PCP       0.86      0.72      0.78      3640\n",
      "        PDEN       0.66      0.70      0.68      1092\n",
      "        PREP       0.89      0.91      0.90     16778\n",
      "    PREP+ADV       0.50      0.61      0.55        31\n",
      "    PREP+ART       0.86      0.99      0.92     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.98      0.77      0.86       309\n",
      "PREP+PROPESS       1.00      0.53      0.69       126\n",
      " PREP+PROSUB       0.52      0.28      0.36       156\n",
      "      PRO-KS       0.64      0.88      0.74      2195\n",
      "      PROADJ       0.85      0.81      0.83      3419\n",
      "     PROPESS       0.83      0.89      0.86      2876\n",
      "      PROSUB       0.88      0.33      0.48      1567\n",
      "          PU       0.88      0.98      0.93     26904\n",
      "           V       0.90      0.86      0.88     19711\n",
      "\n",
      "    accuracy                           0.80    178373\n",
      "   macro avg       0.71      0.63      0.65    178373\n",
      "weighted avg       0.80      0.80      0.79    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(74000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_90 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_90 >= 0.88) and (kappa_90 <= 0.92):\n",
    "    y_pred_rf90, y_pred_nb90, y_pred_lr90, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf90, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb90, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr90, average='macro'))\n",
    "\n",
    "    del y_pred_rf90, y_pred_nb90, y_pred_lr90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd15353",
   "metadata": {},
   "source": [
    "# Agreement 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d51476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.7881975391919005 \n",
      "\n",
      "Kappa Fleiss:  0.7881419499313597 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.85      0.68      0.75      8554\n",
      "         ADV       0.80      0.79      0.79      5446\n",
      "      ADV-KS       0.49      0.54      0.51       230\n",
      "         ART       0.71      0.82      0.76     12580\n",
      "         CUR       0.39      0.81      0.52       296\n",
      "          IN       0.04      0.26      0.07        98\n",
      "          KC       0.67      0.51      0.58      4531\n",
      "          KS       0.80      0.63      0.70      2538\n",
      "           N       0.87      0.85      0.86     36542\n",
      "       NPROP       0.78      0.68      0.73     15936\n",
      "         NUM       0.83      0.80      0.82      2541\n",
      "         PCP       0.90      0.86      0.88      3640\n",
      "        PDEN       0.59      0.76      0.66      1092\n",
      "        PREP       0.90      0.89      0.90     16778\n",
      "    PREP+ADV       0.20      0.87      0.33        31\n",
      "    PREP+ART       0.82      0.93      0.87     10219\n",
      " PREP+PRO-KS       0.08      0.07      0.07        58\n",
      " PREP+PROADJ       0.67      0.93      0.78       309\n",
      "PREP+PROPESS       0.59      0.87      0.71       126\n",
      " PREP+PROSUB       0.17      0.36      0.23       156\n",
      "      PRO-KS       0.75      0.85      0.79      2195\n",
      "      PROADJ       0.83      0.89      0.86      3419\n",
      "     PROPESS       0.78      0.90      0.84      2876\n",
      "      PROSUB       0.48      0.41      0.45      1567\n",
      "          PU       0.89      0.96      0.93     26904\n",
      "           V       0.95      0.89      0.92     19711\n",
      "\n",
      "    accuracy                           0.84    178373\n",
      "   macro avg       0.65      0.72      0.67    178373\n",
      "weighted avg       0.84      0.84      0.84    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.71      0.69      8554\n",
      "         ADV       0.64      0.78      0.70      5446\n",
      "      ADV-KS       0.49      0.48      0.49       230\n",
      "         ART       0.61      0.41      0.49     12580\n",
      "         CUR       0.45      0.83      0.58       296\n",
      "          IN       0.11      0.14      0.12        98\n",
      "          KC       0.45      0.22      0.29      4531\n",
      "          KS       0.52      0.57      0.54      2538\n",
      "           N       0.82      0.68      0.74     36542\n",
      "       NPROP       0.77      0.55      0.64     15936\n",
      "         NUM       0.70      0.81      0.75      2541\n",
      "         PCP       0.76      0.83      0.79      3640\n",
      "        PDEN       0.46      0.73      0.56      1092\n",
      "        PREP       0.85      0.86      0.85     16778\n",
      "    PREP+ADV       0.42      0.87      0.56        31\n",
      "    PREP+ART       0.75      0.98      0.85     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.80      0.95      0.87       309\n",
      "PREP+PROPESS       0.66      0.88      0.75       126\n",
      " PREP+PROSUB       0.22      0.33      0.26       156\n",
      "      PRO-KS       0.67      0.88      0.76      2195\n",
      "      PROADJ       0.75      0.89      0.81      3419\n",
      "     PROPESS       0.68      0.90      0.78      2876\n",
      "      PROSUB       0.47      0.34      0.40      1567\n",
      "          PU       0.74      0.95      0.83     26904\n",
      "           V       0.85      0.85      0.85     19711\n",
      "\n",
      "    accuracy                           0.75    178373\n",
      "   macro avg       0.59      0.67      0.61    178373\n",
      "weighted avg       0.75      0.75      0.74    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.73      0.51      0.60      8554\n",
      "         ADV       0.70      0.70      0.70      5446\n",
      "      ADV-KS       0.45      0.66      0.54       230\n",
      "         ART       0.61      0.80      0.69     12580\n",
      "         CUR       0.77      0.76      0.76       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.55      0.22      0.31      4531\n",
      "          KS       0.78      0.25      0.37      2538\n",
      "           N       0.78      0.82      0.80     36542\n",
      "       NPROP       0.70      0.56      0.62     15936\n",
      "         NUM       0.83      0.69      0.76      2541\n",
      "         PCP       0.85      0.70      0.77      3640\n",
      "        PDEN       0.65      0.74      0.69      1092\n",
      "        PREP       0.88      0.91      0.89     16778\n",
      "    PREP+ADV       0.78      0.58      0.67        31\n",
      "    PREP+ART       0.80      0.99      0.89     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.97      0.67      0.79       309\n",
      "PREP+PROPESS       0.98      0.48      0.65       126\n",
      " PREP+PROSUB       0.98      0.28      0.43       156\n",
      "      PRO-KS       0.58      0.89      0.70      2195\n",
      "      PROADJ       0.85      0.79      0.82      3419\n",
      "     PROPESS       0.88      0.86      0.87      2876\n",
      "      PROSUB       0.80      0.31      0.45      1567\n",
      "          PU       0.88      0.99      0.93     26904\n",
      "           V       0.89      0.85      0.87     19711\n",
      "\n",
      "    accuracy                           0.79    178373\n",
      "   macro avg       0.72      0.62      0.64    178373\n",
      "weighted avg       0.79      0.79      0.78    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(140000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_80 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_80 >= 0.78) and (kappa_80 <= 0.82):\n",
    "    y_pred_rf80, y_pred_nb80, y_pred_lr80, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf80, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb80, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr80, average='macro'))\n",
    "\n",
    "    del y_pred_rf80, y_pred_nb80, y_pred_lr80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b695305",
   "metadata": {},
   "source": [
    "# Agreement 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9716694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.7000126778839613 \n",
      "\n",
      "Kappa Fleiss:  0.699904649153461 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.85      0.65      0.74      8554\n",
      "         ADV       0.72      0.78      0.75      5446\n",
      "      ADV-KS       0.13      0.54      0.21       230\n",
      "         ART       0.68      0.81      0.74     12580\n",
      "         CUR       0.22      0.81      0.35       296\n",
      "          IN       0.03      0.24      0.06        98\n",
      "          KC       0.68      0.48      0.56      4531\n",
      "          KS       0.81      0.62      0.70      2538\n",
      "           N       0.86      0.81      0.84     36542\n",
      "       NPROP       0.78      0.64      0.70     15936\n",
      "         NUM       0.78      0.76      0.77      2541\n",
      "         PCP       0.88      0.81      0.84      3640\n",
      "        PDEN       0.54      0.75      0.63      1092\n",
      "        PREP       0.83      0.87      0.85     16778\n",
      "    PREP+ADV       0.12      0.90      0.22        31\n",
      "    PREP+ART       0.83      0.90      0.86     10219\n",
      " PREP+PRO-KS       0.02      0.07      0.03        58\n",
      " PREP+PROADJ       0.67      0.89      0.77       309\n",
      "PREP+PROPESS       0.56      0.79      0.66       126\n",
      " PREP+PROSUB       0.14      0.26      0.18       156\n",
      "      PRO-KS       0.73      0.84      0.78      2195\n",
      "      PROADJ       0.83      0.87      0.85      3419\n",
      "     PROPESS       0.65      0.89      0.75      2876\n",
      "      PROSUB       0.50      0.40      0.45      1567\n",
      "          PU       0.88      0.94      0.91     26904\n",
      "           V       0.94      0.86      0.90     19711\n",
      "\n",
      "    accuracy                           0.81    178373\n",
      "   macro avg       0.60      0.70      0.62    178373\n",
      "weighted avg       0.82      0.81      0.81    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.69      0.68      8554\n",
      "         ADV       0.61      0.76      0.68      5446\n",
      "      ADV-KS       0.08      0.48      0.14       230\n",
      "         ART       0.54      0.39      0.46     12580\n",
      "         CUR       0.42      0.81      0.55       296\n",
      "          IN       0.02      0.14      0.04        98\n",
      "          KC       0.43      0.22      0.29      4531\n",
      "          KS       0.46      0.51      0.48      2538\n",
      "           N       0.82      0.64      0.72     36542\n",
      "       NPROP       0.78      0.51      0.61     15936\n",
      "         NUM       0.67      0.83      0.74      2541\n",
      "         PCP       0.80      0.80      0.80      3640\n",
      "        PDEN       0.45      0.72      0.56      1092\n",
      "        PREP       0.78      0.84      0.81     16778\n",
      "    PREP+ADV       0.26      0.90      0.41        31\n",
      "    PREP+ART       0.76      0.99      0.86     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.82      0.95      0.88       309\n",
      "PREP+PROPESS       0.76      0.87      0.81       126\n",
      " PREP+PROSUB       0.17      0.33      0.22       156\n",
      "      PRO-KS       0.63      0.89      0.74      2195\n",
      "      PROADJ       0.73      0.89      0.81      3419\n",
      "     PROPESS       0.63      0.84      0.72      2876\n",
      "      PROSUB       0.44      0.34      0.38      1567\n",
      "          PU       0.74      0.94      0.83     26904\n",
      "           V       0.84      0.83      0.84     19711\n",
      "\n",
      "    accuracy                           0.73    178373\n",
      "   macro avg       0.55      0.66      0.58    178373\n",
      "weighted avg       0.73      0.73      0.72    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.72      0.53      0.61      8554\n",
      "         ADV       0.65      0.75      0.70      5446\n",
      "      ADV-KS       0.35      0.66      0.46       230\n",
      "         ART       0.57      0.83      0.68     12580\n",
      "         CUR       0.69      0.69      0.69       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.58      0.21      0.31      4531\n",
      "          KS       0.79      0.30      0.44      2538\n",
      "           N       0.79      0.78      0.79     36542\n",
      "       NPROP       0.72      0.54      0.62     15936\n",
      "         NUM       0.83      0.66      0.74      2541\n",
      "         PCP       0.82      0.67      0.74      3640\n",
      "        PDEN       0.67      0.67      0.67      1092\n",
      "        PREP       0.84      0.90      0.87     16778\n",
      "    PREP+ADV       0.36      0.61      0.45        31\n",
      "    PREP+ART       0.82      0.98      0.89     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.88      0.67      0.76       309\n",
      "PREP+PROPESS       0.84      0.47      0.60       126\n",
      " PREP+PROSUB       0.83      0.28      0.41       156\n",
      "      PRO-KS       0.60      0.88      0.72      2195\n",
      "      PROADJ       0.83      0.81      0.82      3419\n",
      "     PROPESS       0.81      0.87      0.84      2876\n",
      "      PROSUB       0.80      0.32      0.45      1567\n",
      "          PU       0.87      0.99      0.93     26904\n",
      "           V       0.91      0.84      0.87     19711\n",
      "\n",
      "    accuracy                           0.78    178373\n",
      "   macro avg       0.68      0.61      0.62    178373\n",
      "weighted avg       0.78      0.78      0.77    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(200000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_70 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_70 >= 0.68) and (kappa_70 <= 0.72):\n",
    "    y_pred_rf70, y_pred_nb70, y_pred_lr70, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf70, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb70, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr70, average='macro'))\n",
    "\n",
    "    del y_pred_rf70, y_pred_nb70, y_pred_lr70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079adc56",
   "metadata": {},
   "source": [
    "# Agreement 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7968007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.5995095565081134 \n",
      "\n",
      "Kappa Fleiss:  0.5992960767453432 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.81      0.61      0.70      8554\n",
      "         ADV       0.72      0.76      0.74      5446\n",
      "      ADV-KS       0.12      0.49      0.20       230\n",
      "         ART       0.66      0.80      0.72     12580\n",
      "         CUR       0.15      0.71      0.24       296\n",
      "          IN       0.01      0.33      0.02        98\n",
      "          KC       0.54      0.46      0.50      4531\n",
      "          KS       0.57      0.61      0.59      2538\n",
      "           N       0.87      0.75      0.80     36542\n",
      "       NPROP       0.76      0.61      0.68     15936\n",
      "         NUM       0.78      0.70      0.74      2541\n",
      "         PCP       0.87      0.77      0.81      3640\n",
      "        PDEN       0.52      0.75      0.61      1092\n",
      "        PREP       0.83      0.83      0.83     16778\n",
      "    PREP+ADV       0.09      0.97      0.16        31\n",
      "    PREP+ART       0.77      0.86      0.81     10219\n",
      " PREP+PRO-KS       0.02      0.05      0.02        58\n",
      " PREP+PROADJ       0.58      0.89      0.70       309\n",
      "PREP+PROPESS       0.12      0.78      0.21       126\n",
      " PREP+PROSUB       0.08      0.24      0.12       156\n",
      "      PRO-KS       0.69      0.83      0.75      2195\n",
      "      PROADJ       0.81      0.86      0.84      3419\n",
      "     PROPESS       0.68      0.88      0.77      2876\n",
      "      PROSUB       0.34      0.39      0.36      1567\n",
      "          PU       0.87      0.92      0.89     26904\n",
      "           V       0.94      0.83      0.88     19711\n",
      "\n",
      "    accuracy                           0.77    178373\n",
      "   macro avg       0.55      0.68      0.57    178373\n",
      "weighted avg       0.80      0.77      0.78    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.67      0.67      0.67      8554\n",
      "         ADV       0.61      0.77      0.68      5446\n",
      "      ADV-KS       0.07      0.48      0.13       230\n",
      "         ART       0.51      0.37      0.43     12580\n",
      "         CUR       0.36      0.79      0.50       296\n",
      "          IN       0.00      0.10      0.01        98\n",
      "          KC       0.39      0.21      0.28      4531\n",
      "          KS       0.25      0.15      0.19      2538\n",
      "           N       0.82      0.58      0.68     36542\n",
      "       NPROP       0.78      0.48      0.60     15936\n",
      "         NUM       0.67      0.77      0.72      2541\n",
      "         PCP       0.79      0.79      0.79      3640\n",
      "        PDEN       0.42      0.72      0.53      1092\n",
      "        PREP       0.79      0.86      0.82     16778\n",
      "    PREP+ADV       0.15      0.90      0.26        31\n",
      "    PREP+ART       0.73      0.99      0.84     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.83      0.93      0.88       309\n",
      "PREP+PROPESS       0.18      0.85      0.29       126\n",
      " PREP+PROSUB       0.09      0.32      0.14       156\n",
      "      PRO-KS       0.45      0.90      0.60      2195\n",
      "      PROADJ       0.75      0.89      0.81      3419\n",
      "     PROPESS       0.62      0.87      0.72      2876\n",
      "      PROSUB       0.45      0.32      0.37      1567\n",
      "          PU       0.73      0.95      0.82     26904\n",
      "           V       0.84      0.79      0.82     19711\n",
      "\n",
      "    accuracy                           0.70    178373\n",
      "   macro avg       0.50      0.63      0.52    178373\n",
      "weighted avg       0.72      0.70      0.70    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.70      0.50      0.58      8554\n",
      "         ADV       0.66      0.78      0.71      5446\n",
      "      ADV-KS       0.32      0.66      0.43       230\n",
      "         ART       0.56      0.80      0.66     12580\n",
      "         CUR       0.47      0.65      0.55       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.39      0.21      0.28      4531\n",
      "          KS       0.63      0.30      0.41      2538\n",
      "           N       0.78      0.77      0.77     36542\n",
      "       NPROP       0.74      0.51      0.60     15936\n",
      "         NUM       0.81      0.61      0.70      2541\n",
      "         PCP       0.81      0.66      0.73      3640\n",
      "        PDEN       0.74      0.64      0.68      1092\n",
      "        PREP       0.85      0.90      0.88     16778\n",
      "    PREP+ADV       0.76      0.61      0.68        31\n",
      "    PREP+ART       0.78      0.99      0.87     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.84      0.67      0.74       309\n",
      "PREP+PROPESS       0.41      0.49      0.44       126\n",
      " PREP+PROSUB       0.32      0.28      0.30       156\n",
      "      PRO-KS       0.59      0.85      0.70      2195\n",
      "      PROADJ       0.82      0.80      0.81      3419\n",
      "     PROPESS       0.81      0.87      0.84      2876\n",
      "      PROSUB       0.69      0.32      0.44      1567\n",
      "          PU       0.87      0.98      0.93     26904\n",
      "           V       0.91      0.83      0.87     19711\n",
      "\n",
      "    accuracy                           0.77    178373\n",
      "   macro avg       0.63      0.60      0.60    178373\n",
      "weighted avg       0.77      0.77      0.76    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(270000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_60 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_60 >= 0.58) and (kappa_60 <= 0.62):\n",
    "    y_pred_rf60, y_pred_nb60, y_pred_lr60, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf60, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb60, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr60, average='macro'))\n",
    "\n",
    "    del y_pred_rf60, y_pred_nb60, y_pred_lr60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6120cd6",
   "metadata": {},
   "source": [
    "# Agreement 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37277a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.49967274607911183 \n",
      "\n",
      "Kappa Fleiss:  0.49952652584840157 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.81      0.56      0.66      8554\n",
      "         ADV       0.65      0.75      0.70      5446\n",
      "      ADV-KS       0.06      0.48      0.11       230\n",
      "         ART       0.62      0.78      0.69     12580\n",
      "         CUR       0.10      0.65      0.17       296\n",
      "          IN       0.01      0.26      0.02        98\n",
      "          KC       0.56      0.41      0.47      4531\n",
      "          KS       0.58      0.59      0.58      2538\n",
      "           N       0.86      0.66      0.75     36542\n",
      "       NPROP       0.76      0.57      0.65     15936\n",
      "         NUM       0.55      0.56      0.56      2541\n",
      "         PCP       0.84      0.70      0.76      3640\n",
      "        PDEN       0.40      0.73      0.52      1092\n",
      "        PREP       0.78      0.77      0.78     16778\n",
      "    PREP+ADV       0.06      0.90      0.12        31\n",
      "    PREP+ART       0.74      0.82      0.78     10219\n",
      " PREP+PRO-KS       0.01      0.03      0.01        58\n",
      " PREP+PROADJ       0.38      0.86      0.52       309\n",
      "PREP+PROPESS       0.12      0.77      0.21       126\n",
      " PREP+PROSUB       0.05      0.24      0.09       156\n",
      "      PRO-KS       0.65      0.80      0.72      2195\n",
      "      PROADJ       0.77      0.85      0.81      3419\n",
      "     PROPESS       0.59      0.87      0.71      2876\n",
      "      PROSUB       0.28      0.38      0.32      1567\n",
      "          PU       0.80      0.88      0.84     26904\n",
      "           V       0.90      0.78      0.83     19711\n",
      "\n",
      "    accuracy                           0.72    178373\n",
      "   macro avg       0.50      0.64      0.52    178373\n",
      "weighted avg       0.77      0.72      0.74    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.64      0.65      8554\n",
      "         ADV       0.60      0.73      0.66      5446\n",
      "      ADV-KS       0.09      0.48      0.14       230\n",
      "         ART       0.44      0.36      0.40     12580\n",
      "         CUR       0.17      0.74      0.28       296\n",
      "          IN       0.01      0.15      0.01        98\n",
      "          KC       0.42      0.21      0.28      4531\n",
      "          KS       0.24      0.15      0.19      2538\n",
      "           N       0.82      0.53      0.64     36542\n",
      "       NPROP       0.77      0.47      0.58     15936\n",
      "         NUM       0.52      0.63      0.57      2541\n",
      "         PCP       0.80      0.75      0.77      3640\n",
      "        PDEN       0.32      0.79      0.45      1092\n",
      "        PREP       0.77      0.84      0.80     16778\n",
      "    PREP+ADV       0.23      0.90      0.36        31\n",
      "    PREP+ART       0.72      0.99      0.83     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.38      0.94      0.54       309\n",
      "PREP+PROPESS       0.09      0.84      0.17       126\n",
      " PREP+PROSUB       0.08      0.33      0.13       156\n",
      "      PRO-KS       0.43      0.90      0.58      2195\n",
      "      PROADJ       0.77      0.85      0.81      3419\n",
      "     PROPESS       0.61      0.89      0.73      2876\n",
      "      PROSUB       0.43      0.32      0.37      1567\n",
      "          PU       0.71      0.91      0.80     26904\n",
      "           V       0.79      0.75      0.77     19711\n",
      "\n",
      "    accuracy                           0.67    178373\n",
      "   macro avg       0.46      0.62      0.48    178373\n",
      "weighted avg       0.70      0.67      0.67    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.70      0.47      0.56      8554\n",
      "         ADV       0.59      0.73      0.65      5446\n",
      "      ADV-KS       0.28      0.48      0.35       230\n",
      "         ART       0.53      0.79      0.63     12580\n",
      "         CUR       0.51      0.63      0.56       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.49      0.22      0.30      4531\n",
      "          KS       0.50      0.17      0.25      2538\n",
      "           N       0.77      0.70      0.73     36542\n",
      "       NPROP       0.67      0.50      0.57     15936\n",
      "         NUM       0.76      0.48      0.58      2541\n",
      "         PCP       0.79      0.63      0.70      3640\n",
      "        PDEN       0.49      0.70      0.58      1092\n",
      "        PREP       0.82      0.87      0.85     16778\n",
      "    PREP+ADV       0.34      0.61      0.44        31\n",
      "    PREP+ART       0.77      0.99      0.87     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.74      0.70      0.72       309\n",
      "PREP+PROPESS       0.35      0.48      0.41       126\n",
      " PREP+PROSUB       0.17      0.28      0.21       156\n",
      "      PRO-KS       0.51      0.89      0.65      2195\n",
      "      PROADJ       0.83      0.73      0.78      3419\n",
      "     PROPESS       0.74      0.88      0.80      2876\n",
      "      PROSUB       0.56      0.31      0.40      1567\n",
      "          PU       0.84      0.99      0.91     26904\n",
      "           V       0.90      0.81      0.86     19711\n",
      "\n",
      "    accuracy                           0.74    178373\n",
      "   macro avg       0.56      0.58      0.55    178373\n",
      "weighted avg       0.75      0.74      0.73    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(340000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_50 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_50 >= 0.48) and (kappa_50 <= 0.52):\n",
    "    y_pred_rf50, y_pred_nb50, y_pred_lr50, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf50, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb50, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr50, average='macro'))\n",
    "\n",
    "    del y_pred_rf50, y_pred_nb50, y_pred_lr50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d5544",
   "metadata": {},
   "source": [
    "# Agreement 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63528327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.38680033126754876 \n",
      "\n",
      "Kappa Fleiss:  0.3864076035320932 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.77      0.47      0.58      8554\n",
      "         ADV       0.57      0.72      0.64      5446\n",
      "      ADV-KS       0.07      0.48      0.12       230\n",
      "         ART       0.59      0.75      0.66     12580\n",
      "         CUR       0.01      0.13      0.01       296\n",
      "          IN       0.01      0.26      0.01        98\n",
      "          KC       0.45      0.37      0.41      4531\n",
      "          KS       0.38      0.55      0.45      2538\n",
      "           N       0.84      0.53      0.65     36542\n",
      "       NPROP       0.69      0.47      0.56     15936\n",
      "         NUM       0.42      0.39      0.41      2541\n",
      "         PCP       0.74      0.56      0.64      3640\n",
      "        PDEN       0.24      0.68      0.36      1092\n",
      "        PREP       0.70      0.68      0.69     16778\n",
      "    PREP+ADV       0.04      0.87      0.08        31\n",
      "    PREP+ART       0.66      0.74      0.70     10219\n",
      " PREP+PRO-KS       0.01      0.07      0.01        58\n",
      " PREP+PROADJ       0.31      0.82      0.45       309\n",
      "PREP+PROPESS       0.06      0.79      0.11       126\n",
      " PREP+PROSUB       0.04      0.22      0.07       156\n",
      "      PRO-KS       0.47      0.78      0.59      2195\n",
      "      PROADJ       0.77      0.81      0.79      3419\n",
      "     PROPESS       0.51      0.85      0.64      2876\n",
      "      PROSUB       0.20      0.40      0.26      1567\n",
      "          PU       0.78      0.80      0.79     26904\n",
      "           V       0.89      0.69      0.78     19711\n",
      "\n",
      "    accuracy                           0.63    178373\n",
      "   macro avg       0.43      0.57      0.44    178373\n",
      "weighted avg       0.72      0.63      0.66    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.66      0.58      0.62      8554\n",
      "         ADV       0.56      0.66      0.61      5446\n",
      "      ADV-KS       0.08      0.47      0.14       230\n",
      "         ART       0.42      0.36      0.39     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.01      0.15      0.01        98\n",
      "          KC       0.36      0.21      0.27      4531\n",
      "          KS       0.26      0.19      0.22      2538\n",
      "           N       0.81      0.44      0.57     36542\n",
      "       NPROP       0.74      0.41      0.53     15936\n",
      "         NUM       0.42      0.44      0.43      2541\n",
      "         PCP       0.81      0.68      0.74      3640\n",
      "        PDEN       0.28      0.70      0.40      1092\n",
      "        PREP       0.77      0.78      0.78     16778\n",
      "    PREP+ADV       0.02      0.23      0.03        31\n",
      "    PREP+ART       0.68      0.98      0.80     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.34      0.86      0.49       309\n",
      "PREP+PROPESS       0.04      0.75      0.08       126\n",
      " PREP+PROSUB       0.05      0.33      0.08       156\n",
      "      PRO-KS       0.40      0.89      0.55      2195\n",
      "      PROADJ       0.77      0.83      0.80      3419\n",
      "     PROPESS       0.55      0.85      0.67      2876\n",
      "      PROSUB       0.23      0.33      0.27      1567\n",
      "          PU       0.70      0.90      0.79     26904\n",
      "           V       0.79      0.72      0.75     19711\n",
      "\n",
      "    accuracy                           0.62    178373\n",
      "   macro avg       0.41      0.53      0.42    178373\n",
      "weighted avg       0.68      0.62      0.63    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.68      0.39      0.50      8554\n",
      "         ADV       0.56      0.67      0.61      5446\n",
      "      ADV-KS       0.27      0.47      0.35       230\n",
      "         ART       0.52      0.77      0.62     12580\n",
      "         CUR       0.00      0.02      0.01       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.31      0.22      0.26      4531\n",
      "          KS       0.53      0.31      0.39      2538\n",
      "           N       0.74      0.62      0.67     36542\n",
      "       NPROP       0.62      0.46      0.53     15936\n",
      "         NUM       0.67      0.35      0.46      2541\n",
      "         PCP       0.80      0.52      0.63      3640\n",
      "        PDEN       0.46      0.57      0.51      1092\n",
      "        PREP       0.77      0.87      0.82     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.73      0.97      0.83     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.54      0.66      0.60       309\n",
      "PREP+PROPESS       0.06      0.23      0.09       126\n",
      " PREP+PROSUB       0.13      0.26      0.18       156\n",
      "      PRO-KS       0.55      0.88      0.68      2195\n",
      "      PROADJ       0.79      0.74      0.76      3419\n",
      "     PROPESS       0.67      0.76      0.71      2876\n",
      "      PROSUB       0.51      0.28      0.36      1567\n",
      "          PU       0.84      0.98      0.90     26904\n",
      "           V       0.89      0.76      0.82     19711\n",
      "\n",
      "    accuracy                           0.70    178373\n",
      "   macro avg       0.49      0.49      0.47    178373\n",
      "weighted avg       0.71      0.70      0.70    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(420000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_40 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_40 >= 0.38) and (kappa_40 <= 0.42):\n",
    "    y_pred_rf40, y_pred_nb40, y_pred_lr40, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf40, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb40, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr40, average='macro'))\n",
    "\n",
    "    del y_pred_rf40, y_pred_nb40, y_pred_lr40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ebec1",
   "metadata": {},
   "source": [
    "# Agreement 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434b323c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.30339510508302003 \n",
      "\n",
      "Kappa Fleiss:  0.30311698992883257 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.72      0.39      0.51      8554\n",
      "         ADV       0.51      0.66      0.57      5446\n",
      "      ADV-KS       0.05      0.48      0.09       230\n",
      "         ART       0.64      0.73      0.68     12580\n",
      "         CUR       0.01      0.10      0.01       296\n",
      "          IN       0.00      0.14      0.01        98\n",
      "          KC       0.36      0.34      0.35      4531\n",
      "          KS       0.36      0.42      0.39      2538\n",
      "           N       0.82      0.46      0.58     36542\n",
      "       NPROP       0.61      0.41      0.49     15936\n",
      "         NUM       0.21      0.27      0.24      2541\n",
      "         PCP       0.68      0.48      0.56      3640\n",
      "        PDEN       0.28      0.68      0.39      1092\n",
      "        PREP       0.68      0.62      0.65     16778\n",
      "    PREP+ADV       0.03      0.87      0.06        31\n",
      "    PREP+ART       0.60      0.69      0.64     10219\n",
      " PREP+PRO-KS       0.00      0.05      0.01        58\n",
      " PREP+PROADJ       0.14      0.80      0.24       309\n",
      "PREP+PROPESS       0.03      0.62      0.06       126\n",
      " PREP+PROSUB       0.03      0.14      0.04       156\n",
      "      PRO-KS       0.45      0.75      0.57      2195\n",
      "      PROADJ       0.69      0.77      0.73      3419\n",
      "     PROPESS       0.52      0.84      0.64      2876\n",
      "      PROSUB       0.10      0.38      0.15      1567\n",
      "          PU       0.75      0.76      0.75     26904\n",
      "           V       0.81      0.61      0.70     19711\n",
      "\n",
      "    accuracy                           0.58    178373\n",
      "   macro avg       0.39      0.52      0.39    178373\n",
      "weighted avg       0.68      0.58      0.61    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.67      0.53      0.59      8554\n",
      "         ADV       0.48      0.65      0.55      5446\n",
      "      ADV-KS       0.07      0.47      0.12       230\n",
      "         ART       0.53      0.36      0.43     12580\n",
      "         CUR       0.01      0.11      0.02       296\n",
      "          IN       0.00      0.08      0.00        98\n",
      "          KC       0.30      0.21      0.25      4531\n",
      "          KS       0.15      0.15      0.15      2538\n",
      "           N       0.79      0.41      0.54     36542\n",
      "       NPROP       0.64      0.38      0.48     15936\n",
      "         NUM       0.27      0.18      0.22      2541\n",
      "         PCP       0.76      0.59      0.66      3640\n",
      "        PDEN       0.28      0.69      0.40      1092\n",
      "        PREP       0.72      0.79      0.76     16778\n",
      "    PREP+ADV       0.00      0.03      0.00        31\n",
      "    PREP+ART       0.64      0.96      0.77     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.14      0.68      0.23       309\n",
      "PREP+PROPESS       0.07      0.63      0.13       126\n",
      " PREP+PROSUB       0.06      0.33      0.10       156\n",
      "      PRO-KS       0.36      0.88      0.51      2195\n",
      "      PROADJ       0.74      0.83      0.78      3419\n",
      "     PROPESS       0.53      0.87      0.66      2876\n",
      "      PROSUB       0.10      0.29      0.15      1567\n",
      "          PU       0.67      0.83      0.74     26904\n",
      "           V       0.75      0.61      0.67     19711\n",
      "\n",
      "    accuracy                           0.58    178373\n",
      "   macro avg       0.37      0.48      0.38    178373\n",
      "weighted avg       0.65      0.58      0.59    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.61      0.32      0.42      8554\n",
      "         ADV       0.55      0.62      0.58      5446\n",
      "      ADV-KS       0.18      0.47      0.26       230\n",
      "         ART       0.55      0.72      0.62     12580\n",
      "         CUR       0.00      0.01      0.00       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.31      0.24      0.27      4531\n",
      "          KS       0.49      0.21      0.29      2538\n",
      "           N       0.71      0.57      0.63     36542\n",
      "       NPROP       0.64      0.43      0.52     15936\n",
      "         NUM       0.49      0.28      0.36      2541\n",
      "         PCP       0.82      0.45      0.58      3640\n",
      "        PDEN       0.51      0.62      0.56      1092\n",
      "        PREP       0.72      0.83      0.77     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.72      0.94      0.82     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.43      0.69      0.53       309\n",
      "PREP+PROPESS       0.03      0.23      0.06       126\n",
      " PREP+PROSUB       0.48      0.26      0.34       156\n",
      "      PRO-KS       0.48      0.88      0.63      2195\n",
      "      PROADJ       0.77      0.64      0.70      3419\n",
      "     PROPESS       0.62      0.87      0.73      2876\n",
      "      PROSUB       0.15      0.31      0.21      1567\n",
      "          PU       0.81      0.96      0.88     26904\n",
      "           V       0.85      0.73      0.78     19711\n",
      "\n",
      "    accuracy                           0.67    178373\n",
      "   macro avg       0.46      0.47      0.44    178373\n",
      "weighted avg       0.69      0.67      0.66    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(480000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_30 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_30 >= 0.28) and (kappa_30 <= 0.32):\n",
    "    y_pred_rf30, y_pred_nb30, y_pred_lr30, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "\n",
    "    rf_f1.append(f1_score(target, y_pred_rf30, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb30, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr30, average='macro'))\n",
    "\n",
    "    del y_pred_rf30, y_pred_nb30, y_pred_lr30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b15def",
   "metadata": {},
   "source": [
    "# Agreement 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2d96c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.19811177995172735 \n",
      "\n",
      "Kappa Fleiss:  0.19758868913572028 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.65      0.29      0.40      8554\n",
      "         ADV       0.31      0.60      0.41      5446\n",
      "      ADV-KS       0.03      0.46      0.06       230\n",
      "         ART       0.56      0.68      0.61     12580\n",
      "         CUR       0.00      0.10      0.01       296\n",
      "          IN       0.00      0.17      0.01        98\n",
      "          KC       0.32      0.30      0.31      4531\n",
      "          KS       0.26      0.36      0.30      2538\n",
      "           N       0.79      0.36      0.49     36542\n",
      "       NPROP       0.57      0.27      0.37     15936\n",
      "         NUM       0.11      0.11      0.11      2541\n",
      "         PCP       0.43      0.37      0.40      3640\n",
      "        PDEN       0.16      0.67      0.25      1092\n",
      "        PREP       0.60      0.53      0.56     16778\n",
      "    PREP+ADV       0.03      0.87      0.06        31\n",
      "    PREP+ART       0.64      0.60      0.62     10219\n",
      " PREP+PRO-KS       0.00      0.07      0.01        58\n",
      " PREP+PROADJ       0.13      0.81      0.23       309\n",
      "PREP+PROPESS       0.02      0.49      0.04       126\n",
      " PREP+PROSUB       0.01      0.08      0.02       156\n",
      "      PRO-KS       0.31      0.69      0.43      2195\n",
      "      PROADJ       0.66      0.64      0.65      3419\n",
      "     PROPESS       0.31      0.82      0.45      2876\n",
      "      PROSUB       0.10      0.37      0.16      1567\n",
      "          PU       0.70      0.66      0.68     26904\n",
      "           V       0.72      0.44      0.55     19711\n",
      "\n",
      "    accuracy                           0.48    178373\n",
      "   macro avg       0.32      0.45      0.31    178373\n",
      "weighted avg       0.62      0.48      0.51    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.64      0.41      0.50      8554\n",
      "         ADV       0.31      0.49      0.38      5446\n",
      "      ADV-KS       0.03      0.46      0.06       230\n",
      "         ART       0.44      0.36      0.39     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.18      0.01        98\n",
      "          KC       0.21      0.20      0.20      4531\n",
      "          KS       0.10      0.12      0.11      2538\n",
      "           N       0.79      0.33      0.46     36542\n",
      "       NPROP       0.56      0.27      0.36     15936\n",
      "         NUM       0.04      0.04      0.04      2541\n",
      "         PCP       0.36      0.25      0.29      3640\n",
      "        PDEN       0.10      0.56      0.18      1092\n",
      "        PREP       0.70      0.64      0.67     16778\n",
      "    PREP+ADV       0.01      0.65      0.02        31\n",
      "    PREP+ART       0.65      0.94      0.77     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.07      0.39      0.12       309\n",
      "PREP+PROPESS       0.02      0.41      0.03       126\n",
      " PREP+PROSUB       0.04      0.31      0.07       156\n",
      "      PRO-KS       0.29      0.64      0.39      2195\n",
      "      PROADJ       0.64      0.79      0.70      3419\n",
      "     PROPESS       0.28      0.74      0.40      2876\n",
      "      PROSUB       0.14      0.30      0.19      1567\n",
      "          PU       0.63      0.73      0.68     26904\n",
      "           V       0.70      0.37      0.49     19711\n",
      "\n",
      "    accuracy                           0.47    178373\n",
      "   macro avg       0.30      0.41      0.29    178373\n",
      "weighted avg       0.60      0.47      0.49    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.64      0.26      0.37      8554\n",
      "         ADV       0.32      0.61      0.42      5446\n",
      "      ADV-KS       0.07      0.47      0.13       230\n",
      "         ART       0.49      0.62      0.55     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.15      0.01        98\n",
      "          KC       0.24      0.21      0.23      4531\n",
      "          KS       0.14      0.10      0.12      2538\n",
      "           N       0.72      0.43      0.54     36542\n",
      "       NPROP       0.59      0.30      0.40     15936\n",
      "         NUM       0.32      0.18      0.23      2541\n",
      "         PCP       0.54      0.32      0.40      3640\n",
      "        PDEN       0.21      0.47      0.29      1092\n",
      "        PREP       0.70      0.76      0.73     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.70      0.72      0.71     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.18      0.45      0.25       309\n",
      "PREP+PROPESS       0.01      0.10      0.01       126\n",
      " PREP+PROSUB       0.07      0.24      0.11       156\n",
      "      PRO-KS       0.40      0.83      0.54      2195\n",
      "      PROADJ       0.70      0.63      0.66      3419\n",
      "     PROPESS       0.32      0.84      0.47      2876\n",
      "      PROSUB       0.20      0.27      0.23      1567\n",
      "          PU       0.74      0.93      0.82     26904\n",
      "           V       0.77      0.52      0.62     19711\n",
      "\n",
      "    accuracy                           0.56    178373\n",
      "   macro avg       0.35      0.40      0.34    178373\n",
      "weighted avg       0.63      0.56      0.57    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(555000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_20 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_20 >= 0.18) and (kappa_20 <= 0.22):\n",
    "    y_pred_rf20, y_pred_nb20, y_pred_lr20, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "    \n",
    "    rf_f1.append(f1_score(target, y_pred_rf20, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb20, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr20, average='macro'))\n",
    "\n",
    "    del y_pred_rf20, y_pred_nb20, y_pred_lr20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42c9fe",
   "metadata": {},
   "source": [
    "# Agreement 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143db299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.09979620252023436 \n",
      "\n",
      "Kappa Fleiss:  0.0992827661789798 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.48      0.18      0.26      8554\n",
      "         ADV       0.24      0.38      0.29      5446\n",
      "      ADV-KS       0.02      0.42      0.05       230\n",
      "         ART       0.41      0.40      0.40     12580\n",
      "         CUR       0.00      0.09      0.01       296\n",
      "          IN       0.00      0.11      0.00        98\n",
      "          KC       0.16      0.25      0.20      4531\n",
      "          KS       0.13      0.20      0.16      2538\n",
      "           N       0.73      0.21      0.32     36542\n",
      "       NPROP       0.27      0.14      0.18     15936\n",
      "         NUM       0.03      0.05      0.04      2541\n",
      "         PCP       0.30      0.27      0.28      3640\n",
      "        PDEN       0.11      0.65      0.19      1092\n",
      "        PREP       0.52      0.33      0.41     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.47      0.44      0.46     10219\n",
      " PREP+PRO-KS       0.00      0.02      0.00        58\n",
      " PREP+PROADJ       0.05      0.56      0.09       309\n",
      "PREP+PROPESS       0.00      0.11      0.00       126\n",
      " PREP+PROSUB       0.00      0.03      0.01       156\n",
      "      PRO-KS       0.31      0.64      0.42      2195\n",
      "      PROADJ       0.58      0.46      0.52      3419\n",
      "     PROPESS       0.26      0.54      0.35      2876\n",
      "      PROSUB       0.04      0.23      0.07      1567\n",
      "          PU       0.58      0.53      0.55     26904\n",
      "           V       0.48      0.24      0.32     19711\n",
      "\n",
      "    accuracy                           0.31    178373\n",
      "   macro avg       0.24      0.29      0.21    178373\n",
      "weighted avg       0.48      0.31      0.35    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.49      0.25      0.33      8554\n",
      "         ADV       0.21      0.35      0.26      5446\n",
      "      ADV-KS       0.02      0.43      0.03       230\n",
      "         ART       0.41      0.34      0.37     12580\n",
      "         CUR       0.00      0.01      0.00       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.11      0.22      0.15      4531\n",
      "          KS       0.06      0.09      0.07      2538\n",
      "           N       0.75      0.19      0.30     36542\n",
      "       NPROP       0.22      0.12      0.16     15936\n",
      "         NUM       0.01      0.01      0.01      2541\n",
      "         PCP       0.14      0.09      0.10      3640\n",
      "        PDEN       0.10      0.36      0.16      1092\n",
      "        PREP       0.65      0.30      0.41     16778\n",
      "    PREP+ADV       0.01      0.61      0.02        31\n",
      "    PREP+ART       0.51      0.69      0.59     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.04      0.27      0.07       309\n",
      "PREP+PROPESS       0.00      0.14      0.01       126\n",
      " PREP+PROSUB       0.03      0.33      0.06       156\n",
      "      PRO-KS       0.21      0.59      0.32      2195\n",
      "      PROADJ       0.50      0.61      0.55      3419\n",
      "     PROPESS       0.28      0.61      0.39      2876\n",
      "      PROSUB       0.04      0.18      0.06      1567\n",
      "          PU       0.51      0.54      0.53     26904\n",
      "           V       0.43      0.18      0.25     19711\n",
      "\n",
      "    accuracy                           0.31    178373\n",
      "   macro avg       0.22      0.29      0.20    178373\n",
      "weighted avg       0.47      0.31      0.33    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.44      0.11      0.18      8554\n",
      "         ADV       0.18      0.31      0.23      5446\n",
      "      ADV-KS       0.03      0.45      0.06       230\n",
      "         ART       0.26      0.14      0.18     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.05      0.00        98\n",
      "          KC       0.14      0.57      0.23      4531\n",
      "          KS       0.09      0.10      0.09      2538\n",
      "           N       0.68      0.27      0.38     36542\n",
      "       NPROP       0.28      0.14      0.19     15936\n",
      "         NUM       0.08      0.04      0.05      2541\n",
      "         PCP       0.24      0.13      0.17      3640\n",
      "        PDEN       0.03      0.13      0.05      1092\n",
      "        PREP       0.62      0.44      0.52     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.33      0.25      0.29     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.05      0.28      0.08       309\n",
      "PREP+PROPESS       0.00      0.08      0.00       126\n",
      " PREP+PROSUB       0.04      0.17      0.06       156\n",
      "      PRO-KS       0.38      0.78      0.51      2195\n",
      "      PROADJ       0.63      0.36      0.46      3419\n",
      "     PROPESS       0.45      0.67      0.54      2876\n",
      "      PROSUB       0.02      0.10      0.04      1567\n",
      "          PU       0.64      0.69      0.67     26904\n",
      "           V       0.55      0.30      0.39     19711\n",
      "\n",
      "    accuracy                           0.34    178373\n",
      "   macro avg       0.24      0.25      0.21    178373\n",
      "weighted avg       0.48      0.34      0.37    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(625000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_10 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_10 >= 0.08) and (kappa_10 <= 0.12):\n",
    "    y_pred_rf10, y_pred_nb10, y_pred_lr10, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "    \n",
    "    rf_f1.append(f1_score(target, y_pred_rf10, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb10, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr10, average='macro'))\n",
    "\n",
    "    del y_pred_rf10, y_pred_nb10, y_pred_lr10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ccd47",
   "metadata": {},
   "source": [
    "# Agreement 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a0ce6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  0.01451224345019364 \n",
      "\n",
      "Kappa Fleiss:  0.014069726466334066 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.24      0.06      0.10      8554\n",
      "         ADV       0.03      0.05      0.03      5446\n",
      "      ADV-KS       0.00      0.04      0.00       230\n",
      "         ART       0.04      0.02      0.03     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.01      0.00        98\n",
      "          KC       0.03      0.03      0.03      4531\n",
      "          KS       0.01      0.03      0.02      2538\n",
      "           N       0.51      0.06      0.11     36542\n",
      "       NPROP       0.07      0.04      0.05     15936\n",
      "         NUM       0.01      0.01      0.01      2541\n",
      "         PCP       0.04      0.04      0.04      3640\n",
      "        PDEN       0.01      0.08      0.02      1092\n",
      "        PREP       0.07      0.02      0.04     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.05      0.03      0.04     10219\n",
      " PREP+PRO-KS       0.00      0.03      0.00        58\n",
      " PREP+PROADJ       0.02      0.19      0.04       309\n",
      "PREP+PROPESS       0.00      0.02      0.00       126\n",
      " PREP+PROSUB       0.00      0.02      0.00       156\n",
      "      PRO-KS       0.02      0.04      0.03      2195\n",
      "      PROADJ       0.06      0.05      0.06      3419\n",
      "     PROPESS       0.01      0.04      0.02      2876\n",
      "      PROSUB       0.01      0.12      0.02      1567\n",
      "          PU       0.08      0.03      0.04     26904\n",
      "           V       0.10      0.05      0.07     19711\n",
      "\n",
      "    accuracy                           0.04    178373\n",
      "   macro avg       0.05      0.04      0.03    178373\n",
      "weighted avg       0.16      0.04      0.06    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.12      0.04      0.06      8554\n",
      "         ADV       0.03      0.07      0.04      5446\n",
      "      ADV-KS       0.00      0.00      0.00       230\n",
      "         ART       0.02      0.01      0.02     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.01      0.00        98\n",
      "          KC       0.06      0.08      0.07      4531\n",
      "          KS       0.00      0.01      0.00      2538\n",
      "           N       0.56      0.05      0.10     36542\n",
      "       NPROP       0.11      0.04      0.06     15936\n",
      "         NUM       0.00      0.00      0.00      2541\n",
      "         PCP       0.03      0.02      0.03      3640\n",
      "        PDEN       0.01      0.09      0.01      1092\n",
      "        PREP       0.02      0.01      0.01     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.09      0.09      0.09     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.01      0.08      0.01       309\n",
      "PREP+PROPESS       0.00      0.01      0.00       126\n",
      " PREP+PROSUB       0.01      0.24      0.02       156\n",
      "      PRO-KS       0.00      0.01      0.00      2195\n",
      "      PROADJ       0.02      0.01      0.01      3419\n",
      "     PROPESS       0.00      0.01      0.01      2876\n",
      "      PROSUB       0.00      0.01      0.00      1567\n",
      "          PU       0.04      0.01      0.02     26904\n",
      "           V       0.06      0.02      0.04     19711\n",
      "\n",
      "    accuracy                           0.03    178373\n",
      "   macro avg       0.05      0.04      0.02    178373\n",
      "weighted avg       0.16      0.03      0.05    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.05      0.00      0.01      8554\n",
      "         ADV       0.02      0.20      0.04      5446\n",
      "      ADV-KS       0.00      0.00      0.00       230\n",
      "         ART       0.02      0.01      0.02     12580\n",
      "         CUR       0.00      0.01      0.00       296\n",
      "          IN       0.00      0.01      0.00        98\n",
      "          KC       0.02      0.01      0.01      4531\n",
      "          KS       0.00      0.00      0.00      2538\n",
      "           N       0.47      0.03      0.05     36542\n",
      "       NPROP       0.08      0.03      0.04     15936\n",
      "         NUM       0.01      0.01      0.01      2541\n",
      "         PCP       0.01      0.00      0.00      3640\n",
      "        PDEN       0.02      0.07      0.03      1092\n",
      "        PREP       0.04      0.01      0.02     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.01      0.00      0.01     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.00      0.01      0.00       309\n",
      "PREP+PROPESS       0.00      0.05      0.00       126\n",
      " PREP+PROSUB       0.01      0.15      0.02       156\n",
      "      PRO-KS       0.00      0.00      0.00      2195\n",
      "      PROADJ       0.06      0.02      0.03      3419\n",
      "     PROPESS       0.01      0.02      0.01      2876\n",
      "      PROSUB       0.01      0.06      0.01      1567\n",
      "          PU       0.01      0.00      0.00     26904\n",
      "           V       0.11      0.06      0.08     19711\n",
      "\n",
      "    accuracy                           0.03    178373\n",
      "   macro avg       0.04      0.03      0.02    178373\n",
      "weighted avg       0.13      0.03      0.03    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(685000):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_0 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_0 <= 0.02):\n",
    "    y_pred_rf0, y_pred_nb0, y_pred_lr0, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "    \n",
    "    rf_f1.append(f1_score(target, y_pred_rf0, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb0, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr0, average='macro'))\n",
    "\n",
    "    del y_pred_rf0, y_pred_nb0, y_pred_lr0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf21c8e",
   "metadata": {},
   "source": [
    "# Disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fd1220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Cohen:  -0.04761689682657044 \n",
      "\n",
      "Kappa Fleiss:  -0.04815730399699565 \n",
      "\n",
      "Classification report RF: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.01      0.00      0.00      8554\n",
      "         ADV       0.00      0.01      0.00      5446\n",
      "      ADV-KS       0.00      0.00      0.00       230\n",
      "         ART       0.00      0.00      0.00     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.03      0.00        98\n",
      "          KC       0.00      0.00      0.00      4531\n",
      "          KS       0.00      0.01      0.00      2538\n",
      "           N       0.06      0.00      0.01     36542\n",
      "       NPROP       0.02      0.01      0.01     15936\n",
      "         NUM       0.00      0.00      0.00      2541\n",
      "         PCP       0.00      0.00      0.00      3640\n",
      "        PDEN       0.00      0.02      0.00      1092\n",
      "        PREP       0.00      0.00      0.00     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.00      0.00      0.00     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.00      0.00      0.00       309\n",
      "PREP+PROPESS       0.00      0.00      0.00       126\n",
      " PREP+PROSUB       0.00      0.00      0.00       156\n",
      "      PRO-KS       0.00      0.00      0.00      2195\n",
      "      PROADJ       0.01      0.01      0.01      3419\n",
      "     PROPESS       0.00      0.00      0.00      2876\n",
      "      PROSUB       0.01      0.07      0.01      1567\n",
      "          PU       0.00      0.00      0.00     26904\n",
      "           V       0.00      0.00      0.00     19711\n",
      "\n",
      "    accuracy                           0.00    178373\n",
      "   macro avg       0.00      0.01      0.00    178373\n",
      "weighted avg       0.02      0.00      0.00    178373\n",
      "\n",
      "Classification report NB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.00      0.00      0.00      8554\n",
      "         ADV       0.00      0.00      0.00      5446\n",
      "      ADV-KS       0.00      0.00      0.00       230\n",
      "         ART       0.00      0.00      0.00     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.02      0.00        98\n",
      "          KC       0.01      0.01      0.01      4531\n",
      "          KS       0.00      0.00      0.00      2538\n",
      "           N       0.12      0.01      0.02     36542\n",
      "       NPROP       0.02      0.03      0.02     15936\n",
      "         NUM       0.00      0.00      0.00      2541\n",
      "         PCP       0.00      0.00      0.00      3640\n",
      "        PDEN       0.00      0.00      0.00      1092\n",
      "        PREP       0.00      0.00      0.00     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.00      0.00      0.00     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.00      0.00      0.00       309\n",
      "PREP+PROPESS       0.00      0.01      0.00       126\n",
      " PREP+PROSUB       0.00      0.01      0.00       156\n",
      "      PRO-KS       0.00      0.00      0.00      2195\n",
      "      PROADJ       0.00      0.00      0.00      3419\n",
      "     PROPESS       0.00      0.01      0.00      2876\n",
      "      PROSUB       0.01      0.06      0.01      1567\n",
      "          PU       0.00      0.00      0.00     26904\n",
      "           V       0.01      0.01      0.01     19711\n",
      "\n",
      "    accuracy                           0.01    178373\n",
      "   macro avg       0.01      0.01      0.00    178373\n",
      "weighted avg       0.03      0.01      0.01    178373\n",
      "\n",
      "Classification report LR: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.00      0.00      0.00      8554\n",
      "         ADV       0.00      0.01      0.01      5446\n",
      "      ADV-KS       0.00      0.00      0.00       230\n",
      "         ART       0.00      0.00      0.00     12580\n",
      "         CUR       0.00      0.00      0.00       296\n",
      "          IN       0.00      0.00      0.00        98\n",
      "          KC       0.00      0.00      0.00      4531\n",
      "          KS       0.00      0.00      0.00      2538\n",
      "           N       0.08      0.01      0.01     36542\n",
      "       NPROP       0.02      0.00      0.01     15936\n",
      "         NUM       0.00      0.00      0.00      2541\n",
      "         PCP       0.00      0.00      0.00      3640\n",
      "        PDEN       0.00      0.02      0.00      1092\n",
      "        PREP       0.00      0.00      0.00     16778\n",
      "    PREP+ADV       0.00      0.00      0.00        31\n",
      "    PREP+ART       0.00      0.00      0.00     10219\n",
      " PREP+PRO-KS       0.00      0.00      0.00        58\n",
      " PREP+PROADJ       0.00      0.00      0.00       309\n",
      "PREP+PROPESS       0.00      0.02      0.00       126\n",
      " PREP+PROSUB       0.00      0.00      0.00       156\n",
      "      PRO-KS       0.00      0.00      0.00      2195\n",
      "      PROADJ       0.00      0.00      0.00      3419\n",
      "     PROPESS       0.00      0.00      0.00      2876\n",
      "      PROSUB       0.01      0.07      0.02      1567\n",
      "          PU       0.00      0.00      0.00     26904\n",
      "           V       0.00      0.00      0.00     19711\n",
      "\n",
      "    accuracy                           0.00    178373\n",
      "   macro avg       0.00      0.01      0.00    178373\n",
      "weighted avg       0.02      0.00      0.00    178373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "sentence = 0\n",
    "for lists in tuple_list:\n",
    "    for tuples in lists:\n",
    "        data.append({'sentence': sentence, 'words': tuples[0], 'target': tuples[1], 'rater': tuples[1]})\n",
    "    sentence = sentence + 1\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(class_list)):\n",
    "        if(data[i]['rater'] == 'PREP+PRO-KS'):\n",
    "            data[i]['rater'] = 'ADJ'\n",
    "            break\n",
    "        if(data[i]['rater'] == class_list[j]):\n",
    "            data[i]['rater'] = class_list[j+1]\n",
    "            break\n",
    "    \n",
    "# Create the dataframe from the list of data\n",
    "df_agreement = pd.DataFrame(data)\n",
    "\n",
    "kappa_00 = agreement_calculation(df_agreement, name_columns)\n",
    "\n",
    "tuple_list = new_tuple_list(df_agreement)\n",
    "\n",
    "if (kappa_00 <= 0.02):\n",
    "    y_pred_rf00, y_pred_nb00, y_pred_lr00, target = trainer_fixed_pipeline(tuple_list, tuple_list_test)\n",
    "    \n",
    "    rf_f1.append(f1_score(target, y_pred_rf00, average='macro'))\n",
    "    nb_f1.append(f1_score(target, y_pred_nb00, average='macro'))\n",
    "    lr_f1.append(f1_score(target, y_pred_lr00, average='macro'))\n",
    "\n",
    "    del y_pred_rf00, y_pred_nb00, y_pred_lr00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119a1ba",
   "metadata": {},
   "source": [
    "# Comparison of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c1de78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_f1 = rf_f1[::-1]\n",
    "nb_f1 = nb_f1[::-1]\n",
    "lr_f1 = lr_f1[::-1]\n",
    "\n",
    "# Plot a simple line chart\n",
    "plt.plot(agreements, rf_f1, 'g', label='F1 RF', marker='x')\n",
    "plt.plot(agreements, nb_f1, 'b', label='F1 NB', marker='v')\n",
    "plt.plot(agreements, lr_f1, 'y', label='F1 LR', marker='<')\n",
    "\n",
    "plt.axvline(x=0, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=1, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=3, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=5, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=7, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=9, linestyle='dashed', color='gray', alpha=0.5)\n",
    "plt.axvline(x=11, linestyle='dashed', color='gray', alpha=0.5)\n",
    "\n",
    "y_pos = plt.ylim()[1]\n",
    "\n",
    "plt.text(0, y_pos * 1.05, 'Disagreement', color='black', ha='center')\n",
    "plt.text(1, y_pos * 1.01, 'Chance', color='black', ha='center')\n",
    "plt.text(2, y_pos * 1.05, 'Slight', color='black', ha='center')\n",
    "plt.text(4, y_pos * 1.01, 'Fair', color='black', ha='center')\n",
    "plt.text(6, y_pos * 1.05, 'Moderate', color='black', ha='center')\n",
    "plt.text(8, y_pos * 1.01, 'Substantial', color='black', ha='center')\n",
    "plt.text(10, y_pos * 1.05, 'Almost perfect', color='black', ha='center')\n",
    "plt.text(11, y_pos * 1.01, 'Perfect', color='black', ha='center')\n",
    "\n",
    "plt.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "plt.xlabel('Kappa value', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fdbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
